{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB4UREVyRcpo",
        "outputId": "74e7dd83-92b1-426b-96a2-1c9b96087dc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-ranking\n",
            "  Downloading tensorflow_ranking-0.5.5-py2.py3-none-any.whl (147 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/147.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m143.4/147.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow-ranking) (1.4.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-ranking) (1.25.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-ranking) (1.16.0)\n",
            "Collecting tensorflow-serving-api<3.0.0,>=2.0.0 (from tensorflow-ranking)\n",
            "  Downloading tensorflow_serving_api-2.16.1-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: tensorflow<2.16.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-ranking) (2.15.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.16.0->tensorflow-ranking) (2.15.0)\n",
            "INFO: pip is looking at multiple versions of tensorflow-serving-api to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading tensorflow_serving_api-2.15.1-py2.py3-none-any.whl (26 kB)\n",
            "Collecting tensorflow<2.16.0 (from tensorflow-ranking)\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ml-dtypes~=0.3.1 (from tensorflow<2.16.0->tensorflow-ranking)\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.16.0->tensorflow-ranking) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16.0->tensorflow-ranking) (3.2.2)\n",
            "Installing collected packages: ml-dtypes, tensorflow, tensorflow-serving-api, tensorflow-ranking\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "Successfully installed ml-dtypes-0.3.2 tensorflow-2.15.1 tensorflow-ranking-0.5.5 tensorflow-serving-api-2.15.1\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.2)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Collecting category_encoders\n",
            "  Downloading category_encoders-2.6.3-py2.py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (2.0.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2024.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.4.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (24.0)\n",
            "Installing collected packages: category_encoders\n",
            "Successfully installed category_encoders-2.6.3\n",
            "Collecting rbo\n",
            "  Downloading rbo-0.1.3-py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18 in /usr/local/lib/python3.10/dist-packages (from rbo) (1.25.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.59.0 in /usr/local/lib/python3.10/dist-packages (from rbo) (4.66.2)\n",
            "Installing collected packages: rbo\n",
            "Successfully installed rbo-0.1.3\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow-ranking\n",
        "!pip install tensorflow\n",
        "!pip install category_encoders\n",
        "!pip install rbo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3vphCeMRfgD",
        "outputId": "52df4623-91c0-4178-9ccc-73fc6bc77857"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import rbo\n",
        "import category_encoders as ce\n",
        "from sklearn.datasets import dump_svmlight_file\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "from sklearn.metrics import ndcg_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow_ranking as tfr\n",
        "from tensorflow.keras.layers import Input, Concatenate, Embedding, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "from tensorflow.keras.regularizers import l2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CjR6PWNRiGHZ"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljk_-q0GmvCv"
      },
      "source": [
        "### STANDARD RELEVANCE SCORE\n",
        "- 0 (no clicks)\n",
        "- 1 (click)\n",
        "- 2 (transaction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xUhWCbfaR6QI"
      },
      "outputs": [],
      "source": [
        "X_train_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/train.csv')\n",
        "y_train_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/y_train.csv')\n",
        "\n",
        "X_val_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/vali.csv')\n",
        "y_val_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/y_vali.csv')\n",
        "\n",
        "X_test_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/test.csv')\n",
        "y_test_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/y_test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nv6k7pJhSBWX"
      },
      "source": [
        "### NDCG METRIC DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EOhWkKq4RiKV"
      },
      "outputs": [],
      "source": [
        "def ndcg_metric_30(y_true, y_pred):\n",
        "    k = 30\n",
        "    y_score = tf.squeeze(y_pred, axis=-1)\n",
        "    y_true = tf.squeeze(y_true, axis=-1)\n",
        "\n",
        "    order = tf.argsort(y_score, direction='DESCENDING')\n",
        "    y_true_sorted = tf.gather(y_true, order)\n",
        "\n",
        "    gain = tf.pow(2.0, y_true_sorted) - 1.0\n",
        "    discounts = tf.math.log(tf.range(tf.shape(y_true_sorted)[0], dtype=tf.float32) + 2.0) / tf.math.log(2.0)\n",
        "\n",
        "    dcg = tf.reduce_sum(gain / discounts)\n",
        "\n",
        "    ideal_order = tf.argsort(y_true, direction='DESCENDING')\n",
        "    ideal_y_true = tf.gather(y_true, ideal_order)[:k]\n",
        "    ideal_gain = tf.pow(2.0, ideal_y_true) - 1.0\n",
        "    ideal_dcg = tf.reduce_sum(ideal_gain / discounts[:tf.shape(ideal_gain)[0]])\n",
        "\n",
        "    ndcg = tf.where(tf.equal(ideal_dcg, 0), 0.0, dcg / ideal_dcg)\n",
        "    return ndcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UbjnEY1URtsZ"
      },
      "outputs": [],
      "source": [
        "def ndcg_metric_20(y_true, y_pred):\n",
        "    k = 20\n",
        "    y_score = tf.squeeze(y_pred, axis=-1)\n",
        "    y_true = tf.squeeze(y_true, axis=-1)\n",
        "\n",
        "    order = tf.argsort(y_score, direction='DESCENDING')\n",
        "    y_true_sorted = tf.gather(y_true, order)\n",
        "\n",
        "    gain = tf.pow(2.0, y_true_sorted) - 1.0\n",
        "    discounts = tf.math.log(tf.range(tf.shape(y_true_sorted)[0], dtype=tf.float32) + 2.0) / tf.math.log(2.0)\n",
        "\n",
        "    dcg = tf.reduce_sum(gain / discounts)\n",
        "\n",
        "    ideal_order = tf.argsort(y_true, direction='DESCENDING')\n",
        "    ideal_y_true = tf.gather(y_true, ideal_order)[:k]\n",
        "    ideal_gain = tf.pow(2.0, ideal_y_true) - 1.0\n",
        "    ideal_dcg = tf.reduce_sum(ideal_gain / discounts[:tf.shape(ideal_gain)[0]])\n",
        "\n",
        "    ndcg = tf.where(tf.equal(ideal_dcg, 0), 0.0, dcg / ideal_dcg)\n",
        "    return ndcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "qrzgjO_tR0wI"
      },
      "outputs": [],
      "source": [
        "def ndcg_metric_10(y_true, y_pred):\n",
        "    k = 10\n",
        "    y_score = tf.squeeze(y_pred, axis=-1)\n",
        "    y_true = tf.squeeze(y_true, axis=-1)\n",
        "\n",
        "    order = tf.argsort(y_score, direction='DESCENDING')\n",
        "    y_true_sorted = tf.gather(y_true, order)\n",
        "\n",
        "    gain = tf.pow(2.0, y_true_sorted) - 1.0\n",
        "    discounts = tf.math.log(tf.range(tf.shape(y_true_sorted)[0], dtype=tf.float32) + 2.0) / tf.math.log(2.0)\n",
        "\n",
        "    dcg = tf.reduce_sum(gain / discounts)\n",
        "\n",
        "    ideal_order = tf.argsort(y_true, direction='DESCENDING')\n",
        "    ideal_y_true = tf.gather(y_true, ideal_order)[:k]\n",
        "    ideal_gain = tf.pow(2.0, ideal_y_true) - 1.0\n",
        "    ideal_dcg = tf.reduce_sum(ideal_gain / discounts[:tf.shape(ideal_gain)[0]])\n",
        "\n",
        "    ndcg = tf.where(tf.equal(ideal_dcg, 0), 0.0, dcg / ideal_dcg)\n",
        "    return ndcg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "h6Mo9KDHR28v"
      },
      "outputs": [],
      "source": [
        "def ndcg_metric_5(y_true, y_pred):\n",
        "    k = 5\n",
        "    y_score = tf.squeeze(y_pred, axis=-1)\n",
        "    y_true = tf.squeeze(y_true, axis=-1)\n",
        "\n",
        "    order = tf.argsort(y_score, direction='DESCENDING')\n",
        "    y_true_sorted = tf.gather(y_true, order)\n",
        "\n",
        "    gain = tf.pow(2.0, y_true_sorted) - 1.0\n",
        "    discounts = tf.math.log(tf.range(tf.shape(y_true_sorted)[0], dtype=tf.float32) + 2.0) / tf.math.log(2.0)\n",
        "\n",
        "    dcg = tf.reduce_sum(gain / discounts)\n",
        "\n",
        "    ideal_order = tf.argsort(y_true, direction='DESCENDING')\n",
        "    ideal_y_true = tf.gather(y_true, ideal_order)[:k]\n",
        "    ideal_gain = tf.pow(2.0, ideal_y_true) - 1.0\n",
        "    ideal_dcg = tf.reduce_sum(ideal_gain / discounts[:tf.shape(ideal_gain)[0]])\n",
        "\n",
        "    ndcg = tf.where(tf.equal(ideal_dcg, 0), 0.0, dcg / ideal_dcg)\n",
        "    return ndcg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv6tlYWFSFJm"
      },
      "source": [
        "### DATA PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Kcv9wGDhiJc7"
      },
      "outputs": [],
      "source": [
        "X_train_pp = X_train_pp.drop(columns=['rank_noad'])\n",
        "X_val_pp = X_val_pp.drop(columns=['rank_noad'])\n",
        "X_test_pp = X_test_pp.drop(columns=['rank_noad'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "4XbWlYgLSAUr"
      },
      "outputs": [],
      "source": [
        "query_id_train_pp = X_train_pp['qid']\n",
        "query_id_val_pp = X_val_pp['qid']\n",
        "query_id_test_pp = X_test_pp['qid']\n",
        "\n",
        "\n",
        "X_train_2_pp = X_train_pp.drop(columns=['qid'])\n",
        "X_train_arr_pp = X_train_2_pp.values.astype(float)\n",
        "\n",
        "X_val_2_pp = X_val_pp.drop(columns=['qid'])\n",
        "X_val_arr_pp = X_val_2_pp.values.astype(float)\n",
        "\n",
        "X_test_2_pp = X_test_pp.drop(columns=['qid'])\n",
        "X_test_arr_pp = X_test_2_pp.values.astype(float)\n",
        "\n",
        "\n",
        "y_train_array_pp = y_train_pp.values.astype(int).ravel()\n",
        "y_train_array_pp = y_train_array_pp.flatten().astype(float)\n",
        "\n",
        "y_val_array_pp = y_val_pp.values.astype(int).ravel()\n",
        "y_val_array_pp = y_val_array_pp.flatten().astype(float)\n",
        "\n",
        "y_test_array_pp = y_test_pp.values.astype(int).ravel()\n",
        "y_test_array_pp = y_test_array_pp.flatten().astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NEURAL NETWORK ARCHITECTURE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDS_QPO_SRUT",
        "outputId": "572e77ec-d075-4741-f07a-0b0aa07f0aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "13702/13702 [==============================] - 69s 5ms/step - loss: -0.0719 - ndcg_metric_5: 0.3869 - val_loss: -0.0732 - val_ndcg_metric_5: 0.3828\n",
            "Epoch 2/50\n",
            "13702/13702 [==============================] - 67s 5ms/step - loss: -0.0719 - ndcg_metric_5: 0.3869 - val_loss: -0.0732 - val_ndcg_metric_5: 0.3828\n",
            "Epoch 3/50\n",
            "13702/13702 [==============================] - 66s 5ms/step - loss: -0.0719 - ndcg_metric_5: 0.3870 - val_loss: -0.0732 - val_ndcg_metric_5: 0.3828\n",
            "Epoch 4/50\n",
            "13702/13702 [==============================] - 67s 5ms/step - loss: -0.0719 - ndcg_metric_5: 0.3876 - val_loss: -0.0732 - val_ndcg_metric_5: 0.3828\n",
            "Epoch 5/50\n",
            "13702/13702 [==============================] - 69s 5ms/step - loss: -0.0719 - ndcg_metric_5: 0.3880 - val_loss: -0.0732 - val_ndcg_metric_5: 0.3828\n",
            "Epoch 6/50\n",
            "13702/13702 [==============================] - 70s 5ms/step - loss: -0.0719 - ndcg_metric_5: 0.3873 - val_loss: -0.0732 - val_ndcg_metric_5: 0.3828\n"
          ]
        }
      ],
      "source": [
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=5,restore_best_weights=True, monitor='val_ndcg_metric_5', mode='max')\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Input layers for features and search_id\n",
        "input_features = Input(shape=(X_train_arr_pp.shape[1],), name='features')\n",
        "input_query_id = Input(shape=(1,), name='search_id')\n",
        "\n",
        "# Embedding or concatenation layer for search_id\n",
        "concatenated_inputs = Concatenate()([input_features, input_query_id])\n",
        "\n",
        "# Define the rest of your model architecture\n",
        "dense_layer_1 = Dense(128, activation='relu')(concatenated_inputs)\n",
        "batch_norm_1 = BatchNormalization()(dense_layer_1)\n",
        "#dense_layer_2 = Dense(512, activation='relu')(dense_layer_1)\n",
        "#dense_layer_3 = Dense(1024, activation='relu')(dense_layer_2)\n",
        "#dropout_layer = Dropout(0.3)(dense_layer_3)\n",
        "#dense_layer_4 = Dense(512, activation='relu')(dense_layer_3)\n",
        "dense_layer_5 = Dense(256, activation='relu')(dense_layer_1)\n",
        "dense_layer_6 = Dense(128, activation='relu')(dense_layer_5)\n",
        "dense_layer_7 = Dense(64, activation='relu')(dense_layer_6)\n",
        "dense_layer_8 = Dense(32, activation='relu')(dense_layer_7)\n",
        "output_layer = Dense(1)(dense_layer_8)\n",
        "\n",
        "#Losses\n",
        "loss_ndcgapprox = tfr.keras.losses.ApproxNDCGLoss(temperature=0.1)\n",
        "loss_clickeml = tfr.keras.losses.ClickEMLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[input_features, input_query_id], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_ndcgapprox, metrics=[ndcg_metric_5])\n",
        "\n",
        "# Fit the model with both features and search_id\n",
        "history = model.fit({'features': X_train_arr_pp, 'search_id': query_id_train_pp}, y_train_array_pp,\n",
        "                    epochs=50, validation_data=({'features': X_val_arr_pp, 'search_id': query_id_val_pp}, y_val_array_pp),\n",
        "                    verbose=True, batch_size=64, callbacks=[early_stopping_cb])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHPm0H-GSeM5"
      },
      "source": [
        "### NDCG COMPUTATION FOR PREDICTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XuF763zCShQg"
      },
      "outputs": [],
      "source": [
        "def ndcg(y_score, y_true, k):\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    gain = 2 ** y_true - 1\n",
        "\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gain / discounts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQPKBK-ESiQx",
        "outputId": "6b9106d5-6593-4c3e-d708-70de1ed2489d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 12779/12779 [19:56<00:00, 10.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG K5 0.1678415685377167\n",
            "NDCG K10 0.23677712178936325\n",
            "NDCG K20 0.3335979623488874\n",
            "NDCG K30 0.38030904720319686\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = y_test_pp['relevance']\n",
        "query_id_test_pp = X_test_pp['qid']\n",
        "qids = np.unique(query_id_test_pp)\n",
        "\n",
        "ndcg_5 = list()\n",
        "ndcg_10 = list()\n",
        "ndcg_20 = list()\n",
        "ndcg_30 = list()\n",
        "predict_values = list()\n",
        "\n",
        "for i, qid in tqdm(enumerate(qids[:]), total=len(qids)):\n",
        "    query_id_test_pp = X_test_pp['qid']\n",
        "    y = y_test_pred[query_id_test_pp == qid]\n",
        "\n",
        "    if np.sum(y) == 0:\n",
        "        continue\n",
        "\n",
        "    X_test_2_pp = X_test_pp[X_test_pp['qid'] == qid]\n",
        "\n",
        "    query_id_test_pp = X_test_2_pp['qid']\n",
        "\n",
        "    X_test_2_pp = X_test_2_pp.drop(columns=['qid'])\n",
        "    X_test_arr_pp = X_test_2_pp.values.astype(float)\n",
        "\n",
        "    predictions = model.predict({'features': X_test_arr_pp, 'search_id': query_id_test_pp}, verbose=False)\n",
        "    predictions = predictions.flatten()\n",
        "    predict_values.append([predictions])\n",
        "\n",
        "    for i in [5,10,20,30]:\n",
        "      idcg = ndcg(y, y, k=i)\n",
        "      eval(f\"ndcg_{i}\").append(ndcg(predictions, y, k=i) / idcg)\n",
        "\n",
        "\n",
        "print(f'NDCG K5 {np.mean(ndcg_5)}')\n",
        "print(f'NDCG K10 {np.mean(ndcg_10)}')\n",
        "print(f'NDCG K20 {np.mean(ndcg_20)}')\n",
        "print(f'NDCG K30 {np.mean(ndcg_30)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NDCG COMPUTATION FOR PREDICTIONS (first 500 query ID's)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "gDoA6h176N-K"
      },
      "outputs": [],
      "source": [
        "unique_qid = X_test_pp['qid'].unique()\n",
        "testtt = X_test_pp[X_test_pp['qid'].isin(unique_qid[:500])]\n",
        "testtt.shape[0]\n",
        "t_testtt = y_test_pp[:testtt.shape[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4okznCQrM7W",
        "outputId": "6c8c727f-7d5c-412e-8237-b6d6d3d1c846"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [00:38<00:00, 12.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NDCG K5 0.501869321551276\n",
            "NDCG K10 0.5617920037012842\n",
            "NDCG K20 0.6005449305711997\n",
            "NDCG K30 0.6115901741421633\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "y_test_pred = t_testtt['relevance']\n",
        "query_id_test_pp = testtt['qid']\n",
        "qids = np.unique(query_id_test_pp)\n",
        "\n",
        "ndcg_5 = list()\n",
        "ndcg_10 = list()\n",
        "ndcg_20 = list()\n",
        "ndcg_30 = list()\n",
        "predict_values = list()\n",
        "\n",
        "for i, qid in tqdm(enumerate(qids[:]), total=len(qids)):\n",
        "    query_id_test_pp = testtt['qid']\n",
        "    y = y_test_pred[query_id_test_pp == qid]\n",
        "\n",
        "    if np.sum(y) == 0:\n",
        "        continue\n",
        "\n",
        "    X_test_2_pp = testtt[testtt['qid'] == qid]\n",
        "\n",
        "    query_id_test_pp = X_test_2_pp['qid']\n",
        "\n",
        "    X_test_2_pp = X_test_2_pp.drop(columns=['qid'])\n",
        "    X_test_arr_pp = X_test_2_pp.values.astype(float)\n",
        "\n",
        "    predictions = model.predict({'features': X_test_arr_pp, 'search_id': query_id_test_pp}, verbose=False)\n",
        "    predictions = predictions.flatten()\n",
        "    predict_values.append([predictions])\n",
        "\n",
        "    for i in [5,10,20,30]:\n",
        "      idcg = ndcg(y, y, k=i)\n",
        "      eval(f\"ndcg_{i}\").append(ndcg(predictions, y, k=i) / idcg)\n",
        "\n",
        "\n",
        "print(f'NDCG K5 {np.mean(ndcg_5)}')\n",
        "print(f'NDCG K10 {np.mean(ndcg_10)}')\n",
        "print(f'NDCG K20 {np.mean(ndcg_20)}')\n",
        "print(f'NDCG K30 {np.mean(ndcg_30)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MODIFYING RELEVANCE SCORE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQd7wwPam9UU"
      },
      "source": [
        "### PRICE_BUCKET RELEVANCE SCORE\n",
        "Adding to the traditional relevance score, 1 if price bucket is between 1-3, and 2 if 4-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAUDbu_3nNh6"
      },
      "outputs": [],
      "source": [
        "X_train_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/train.csv')\n",
        "y_train_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/y_train.csv')\n",
        "\n",
        "X_val_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/vali.csv')\n",
        "y_val_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/y_vali.csv')\n",
        "\n",
        "X_test_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/test.csv')\n",
        "y_test_pp = pd.read_csv('/content/drive/MyDrive/Expedia/allRankNew/allRank/expedia_data/Millionrows_allRank/target_relevance/y_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtEEhy49nVHB"
      },
      "outputs": [],
      "source": [
        "query_id_train_pp = X_train_pp['qid']\n",
        "query_id_val_pp = X_val_pp['qid']\n",
        "query_id_test_pp = X_test_pp['qid']\n",
        "\n",
        "\n",
        "X_train_2_pp = X_train_pp.drop(columns=['qid'])\n",
        "X_train_arr_pp = X_train_2_pp.values.astype(float)\n",
        "\n",
        "X_val_2_pp = X_val_pp.drop(columns=['qid'])\n",
        "X_val_arr_pp = X_val_2_pp.values.astype(float)\n",
        "\n",
        "X_test_2_pp = X_test_pp.drop(columns=['qid'])\n",
        "X_test_arr_pp = X_test_2_pp.values.astype(float)\n",
        "\n",
        "\n",
        "y_train_array_pp = y_train_pp.values.astype(int).ravel()\n",
        "y_train_array_pp = y_train_array_pp.flatten().astype(float)\n",
        "\n",
        "y_val_array_pp = y_val_pp.values.astype(int).ravel()\n",
        "y_val_array_pp = y_val_array_pp.flatten().astype(float)\n",
        "\n",
        "y_test_array_pp = y_test_pp.values.astype(int).ravel()\n",
        "y_test_array_pp = y_test_array_pp.flatten().astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAMVFrs1nVHB",
        "outputId": "80ba25cc-470b-44c4-b201-86e0ae3d762d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "13702/13702 [==============================] - 174s 12ms/step - loss: -0.0719 - ndcg_metric_30: 0.3614 - ndcg_metric_20: 0.3614 - ndcg_metric_10: 0.3615 - ndcg_metric_5: 0.3868 - val_loss: -0.0732 - val_ndcg_metric_30: 0.3181 - val_ndcg_metric_20: 0.3191 - val_ndcg_metric_10: 0.3260 - val_ndcg_metric_5: 0.3589\n",
            "Epoch 2/50\n",
            "13702/13702 [==============================] - 168s 12ms/step - loss: -0.0719 - ndcg_metric_30: 0.3614 - ndcg_metric_20: 0.3614 - ndcg_metric_10: 0.3616 - ndcg_metric_5: 0.3869 - val_loss: -0.0732 - val_ndcg_metric_30: 0.3181 - val_ndcg_metric_20: 0.3191 - val_ndcg_metric_10: 0.3260 - val_ndcg_metric_5: 0.3589\n",
            "Epoch 3/50\n",
            "13702/13702 [==============================] - 168s 12ms/step - loss: -0.0719 - ndcg_metric_30: 0.3613 - ndcg_metric_20: 0.3613 - ndcg_metric_10: 0.3615 - ndcg_metric_5: 0.3870 - val_loss: -0.0732 - val_ndcg_metric_30: 0.3181 - val_ndcg_metric_20: 0.3191 - val_ndcg_metric_10: 0.3260 - val_ndcg_metric_5: 0.3589\n",
            "Epoch 4/50\n",
            "13702/13702 [==============================] - 169s 12ms/step - loss: -0.0719 - ndcg_metric_30: 0.3620 - ndcg_metric_20: 0.3620 - ndcg_metric_10: 0.3622 - ndcg_metric_5: 0.3876 - val_loss: -0.0732 - val_ndcg_metric_30: 0.3181 - val_ndcg_metric_20: 0.3191 - val_ndcg_metric_10: 0.3260 - val_ndcg_metric_5: 0.3589\n"
          ]
        }
      ],
      "source": [
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True, monitor='val_ndcg_metric_5', mode='max')\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Input layers for features and search_id\n",
        "input_features = Input(shape=(X_train_arr_pp.shape[1],), name='features')\n",
        "input_query_id = Input(shape=(1,), name='search_id')\n",
        "\n",
        "# Embedding or concatenation layer for search_id\n",
        "concatenated_inputs = Concatenate()([input_features, input_query_id])\n",
        "\n",
        "# Define the rest of your model architecture\n",
        "dense_layer_1 = Dense(128, activation='relu')(concatenated_inputs)\n",
        "batch_norm_1 = BatchNormalization()(dense_layer_1)\n",
        "dense_layer_2 = Dense(512, activation='relu')(dense_layer_1)\n",
        "dropout_layer = Dropout(0.3)(dense_layer_2)\n",
        "dense_layer_3 = Dense(1024, activation='relu')(dense_layer_2)\n",
        "dropout_layer = Dropout(0.3)(dense_layer_3)\n",
        "dense_layer_4 = Dense(512, activation='relu')(dense_layer_3)\n",
        "dense_layer_5 = Dense(256, activation='relu')(dense_layer_4)\n",
        "dense_layer_6 = Dense(128, activation='relu')(dense_layer_5)\n",
        "dense_layer_7 = Dense(64, activation='relu')(dense_layer_6)\n",
        "dense_layer_8 = Dense(32, activation='relu')(dense_layer_7)\n",
        "output_layer = Dense(1)(dense_layer_8)\n",
        "\n",
        "#Losses\n",
        "loss_ndcgapprox = tfr.keras.losses.ApproxNDCGLoss(temperature=0.1)\n",
        "loss_clickeml = tfr.keras.losses.ClickEMLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[input_features, input_query_id], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_ndcgapprox, metrics=[ndcg_metric_30, ndcg_metric_20, ndcg_metric_10, ndcg_metric_5])\n",
        "\n",
        "# Fit the model with both features and search_id\n",
        "history = model.fit({'features': X_train_arr_pp, 'search_id': query_id_train_pp}, y_train_array_pp,\n",
        "                    epochs=50, validation_data=({'features': X_val_arr_pp, 'search_id': query_id_val_pp}, y_val_array_pp),\n",
        "                    verbose=True, batch_size=64, callbacks=[early_stopping_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvpmX5iOndYY",
        "outputId": "0679adb5-10fa-4378-99bc-ab647ea02061"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▋      | 4649/12779 [07:22<13:13, 10.25it/s]"
          ]
        }
      ],
      "source": [
        "y_test_pred = y_test_pp['relevance']\n",
        "query_id_test_pp = X_test_pp['qid']\n",
        "qids = np.unique(query_id_test_pp)\n",
        "\n",
        "ndcg_5 = list()\n",
        "ndcg_10 = list()\n",
        "ndcg_20 = list()\n",
        "ndcg_30 = list()\n",
        "predict_values = list()\n",
        "\n",
        "for i, qid in tqdm(enumerate(qids[:]), total=len(qids)):\n",
        "    query_id_test_pp = X_test_pp['qid']\n",
        "    y = y_test_pred[query_id_test_pp == qid]\n",
        "\n",
        "    if np.sum(y) == 0:\n",
        "        continue\n",
        "\n",
        "    X_test_2_pp = X_test_pp[X_test_pp['qid'] == qid]\n",
        "\n",
        "    query_id_test_pp = X_test_2_pp['qid']\n",
        "\n",
        "    X_test_2_pp = X_test_2_pp.drop(columns=['qid'])\n",
        "    X_test_arr_pp = X_test_2_pp.values.astype(float)\n",
        "\n",
        "    predictions = model.predict({'features': X_test_arr_pp, 'search_id': query_id_test_pp}, verbose=False)\n",
        "    predictions = predictions.flatten()\n",
        "    predict_values.append([predictions])\n",
        "\n",
        "    for i in [5,10,20,30]:\n",
        "      idcg = ndcg(y, y, k=i)\n",
        "      eval(f\"ndcg_{i}\").append(ndcg(predictions, y, k=i) / idcg)\n",
        "\n",
        "\n",
        "print(f'NDCG K5 {np.mean(ndcg_5)}')\n",
        "print(f'NDCG K10 {np.mean(ndcg_10)}')\n",
        "print(f'NDCG K20 {np.mean(ndcg_20)}')\n",
        "print(f'NDCG K30 {np.mean(ndcg_30)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEU2O8Vwof9g"
      },
      "source": [
        "### REVIEW RATING - REVIEW COUNT RELEVANCE SCORE\n",
        "\n",
        "- If relevance is not equal to 0:\n",
        "    - If review count is greater than 200 and review rating is 4, sum 2\n",
        "    - If review_count is greater than 200 and review rating is 5, sum 3\n",
        "    - If review count is greater than 200 and review rating is 3 or below, deduct 1\n",
        "    - If review count is lower than 200 but review rating is equal to 4 or 5, add 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M5JL_h5p2Rg"
      },
      "outputs": [],
      "source": [
        "X_train_pp_rr = pd.read_csv('/content/drive/MyDrive/Expedia/Data/workstation_1millionrows/target_combined_click_reviewrating_review_count_relevance/train.csv')\n",
        "y_train_pp_rr = pd.read_csv('/content/drive/MyDrive/Expedia/Data/workstation_1millionrows/target_combined_click_reviewrating_review_count_relevance/y_train.csv')\n",
        "\n",
        "X_val_pp_rr = pd.read_csv('/content/drive/MyDrive/Expedia/Data/workstation_1millionrows/target_combined_click_reviewrating_review_count_relevance/vali.csv')\n",
        "y_val_pp_rr = pd.read_csv('/content/drive/MyDrive/Expedia/Data/workstation_1millionrows/target_combined_click_reviewrating_review_count_relevance/y_vali.csv')\n",
        "\n",
        "X_test_pp_rr = pd.read_csv('/content/drive/MyDrive/Expedia/Data/workstation_1millionrows/target_combined_click_reviewrating_review_count_relevance/test.csv')\n",
        "y_test_pp_rr = pd.read_csv('/content/drive/MyDrive/Expedia/Data/workstation_1millionrows/target_combined_click_reviewrating_review_count_relevance/y_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8VRXxragSMP"
      },
      "outputs": [],
      "source": [
        "X_train_pp_rr = X_train_pp_rr.drop(columns=['prop_id_target'])\n",
        "X_val_pp_rr = X_val_pp_rr.drop(columns=['prop_id_target'])\n",
        "X_test_pp_rr = X_test_pp_rr.drop(columns=['prop_id_target'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZnMgpvUgpNX"
      },
      "outputs": [],
      "source": [
        "query_id_train_pp_rr = X_train_pp_rr['qid']\n",
        "query_id_val_pp_rr = X_val_pp_rr['qid']\n",
        "query_id_test_pp_rr = X_test_pp_rr['qid']\n",
        "\n",
        "\n",
        "X_train_2_pp_rr = X_train_pp_rr.drop(columns=['qid'])\n",
        "X_train_arr_pp_rr = X_train_2_pp_rr.values.astype(float)\n",
        "\n",
        "X_val_2_pp_rr = X_val_pp_rr.drop(columns=['qid'])\n",
        "X_val_arr_pp_rr = X_val_2_pp_rr.values.astype(float)\n",
        "\n",
        "X_test_2_pp_rr = X_test_pp_rr.drop(columns=['qid'])\n",
        "X_test_arr_pp_rr = X_test_2_pp_rr.values.astype(float)\n",
        "\n",
        "\n",
        "y_train_array_pp_rr = y_train_pp_rr.values.astype(int).ravel()\n",
        "y_train_array_pp_rr = y_train_array_pp_rr.flatten().astype(float)\n",
        "\n",
        "y_val_array_pp_rr = y_val_pp_rr.values.astype(int).ravel()\n",
        "y_val_array_pp_rr = y_val_array_pp_rr.flatten().astype(float)\n",
        "\n",
        "y_test_array_pp_rr = y_test_pp_rr.values.astype(int).ravel()\n",
        "y_test_array_pp_rr = y_test_array_pp_rr.flatten().astype(float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq4hZXoEguR2"
      },
      "outputs": [],
      "source": [
        "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True, monitor='val_ndcg_metric_5', mode='max')\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Input layers for features and search_id\n",
        "input_features = Input(shape=(X_train_arr_pp_rr.shape[1],), name='features')\n",
        "input_query_id = Input(shape=(1,), name='search_id')\n",
        "\n",
        "# Embedding or concatenation layer for search_id\n",
        "concatenated_inputs = Concatenate()([input_features, input_query_id])\n",
        "\n",
        "# Define the rest of your model architecture\n",
        "dense_layer_1 = Dense(128, activation='relu')(concatenated_inputs)\n",
        "batch_norm_1 = BatchNormalization()(dense_layer_1)\n",
        "#dense_layer_2 = Dense(512, activation='relu')(dense_layer_1)\n",
        "#dense_layer_3 = Dense(1024, activation='relu')(dense_layer_2)\n",
        "#dropout_layer = Dropout(0.3)(dense_layer_3)\n",
        "#dense_layer_4 = Dense(512, activation='relu')(dense_layer_3)\n",
        "#dense_layer_5 = Dense(256, activation='relu')(dense_layer_4)\n",
        "#dense_layer_6 = Dense(128, activation='relu')(dense_layer_5)\n",
        "dense_layer_7 = Dense(64, activation='relu')(dense_layer_1)\n",
        "dense_layer_8 = Dense(32, activation='relu')(dense_layer_7)\n",
        "output_layer = Dense(1)(dense_layer_8)\n",
        "\n",
        "#Losses\n",
        "loss_ndcgapprox = tfr.keras.losses.ApproxNDCGLoss(temperature=0.1)\n",
        "loss_clickeml = tfr.keras.losses.ClickEMLoss()\n",
        "\n",
        "#Optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[input_features, input_query_id], outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=loss_ndcgapprox, metrics=[ndcg_metric_5])\n",
        "\n",
        "# Fit the model with both features and search_id\n",
        "history = model.fit({'features': X_train_arr_pp_rr, 'search_id': query_id_train_pp_rr}, y_train_array_pp_rr,\n",
        "                    epochs=50, validation_data=({'features': X_val_arr_pp_rr, 'search_id': query_id_val_pp_rr}, y_val_array_pp_rr),\n",
        "                    verbose=True, batch_size=64, callbacks=[early_stopping_cb])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-u2GZxN5g0HA"
      },
      "outputs": [],
      "source": [
        "unique_qid_rr = X_test_pp_rr['qid'].unique()\n",
        "testtt_rr = X_test_pp_rr[X_test_pp_rr['qid'].isin(unique_qid_rr[:3000])]\n",
        "testtt_rr.shape[0]\n",
        "t_testtt_rr = y_test_pp_rr[:testtt_rr.shape[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LA8-JIP-g0HB"
      },
      "outputs": [],
      "source": [
        "y_test_pred_rr = t_testtt_rr['relevance']\n",
        "query_id_test_pp_rr = testtt_rr['qid']\n",
        "qids_rr = np.unique(query_id_test_pp_rr)\n",
        "\n",
        "ndcg_5_rr = list()\n",
        "ndcg_10_rr = list()\n",
        "ndcg_20_rr = list()\n",
        "ndcg_30_rr = list()\n",
        "predict_values_rr = list()\n",
        "\n",
        "for i, qid in tqdm(enumerate(qids_rr[:]), total=len(qids_rr)):\n",
        "    query_id_test_pp_rr = testtt_rr['qid']\n",
        "    y = y_test_pred_rr[query_id_test_pp_rr == qid]\n",
        "\n",
        "    if np.sum(y) == 0:\n",
        "        continue\n",
        "\n",
        "    X_test_2_pp = testtt_rr[testtt_rr['qid'] == qid]\n",
        "\n",
        "    query_id_test_pp_rr = X_test_2_pp['qid']\n",
        "\n",
        "    X_test_2_pp = X_test_2_pp.drop(columns=['qid'])\n",
        "    X_test_arr_pp = X_test_2_pp.values.astype(float)\n",
        "\n",
        "    predictions = model.predict({'features': X_test_arr_pp, 'search_id': query_id_test_pp_rr}, verbose=False)\n",
        "    predictions = predictions.flatten()\n",
        "    predict_values_rr.append([predictions])\n",
        "\n",
        "    for i in [5,10,20,30]:\n",
        "      idcg = ndcg(y, y, k=i)\n",
        "      eval(f\"ndcg_{i}\").append(ndcg(predictions, y, k=i) / idcg)\n",
        "\n",
        "\n",
        "print(f'NDCG K5 {np.mean(ndcg_5_rr)}')\n",
        "print(f'NDCG K10 {np.mean(ndcg_10_rr)}')\n",
        "print(f'NDCG K20 {np.mean(ndcg_20_rr)}')\n",
        "print(f'NDCG K30 {np.mean(ndcg_30_rr)}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
